{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "from collections import namedtuple\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorboard\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR, CyclicLR\n",
    "import torchmetrics\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets, DatasetDict, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import BartConfig, T5Config\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import generate_seq, generate_input_target, generate_from_data, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_path = './data/gogamza-kobart-base-v2_tokenized_dataset'\n",
    "baseline_checkpoint_path = '/home/thesol1/projects/KoGrammar/models/230510-16_32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1016426\n",
      "    })\n",
      "    train_baseline: Dataset({\n",
      "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 508213\n",
      "    })\n",
      "    train_distil: Dataset({\n",
      "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 508212\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 56468\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 56469\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(tokenized_dataset_path):\n",
    "    tokenized_dataset = load_from_disk(tokenized_dataset_path)\n",
    "    print(tokenized_dataset)\n",
    "else:\n",
    "    print(\"Tokenized dataset not found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(baseline_checkpoint_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(baseline_checkpoint_path).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validset = tokenized_dataset['valid'].shuffle()\n",
    "test_sample = validset.shuffle().select(range(1))\n",
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "누나랑 연락하는거 조아여!@@\n",
      "\n",
      "누나랑 연락하는 거 좋아! @@\n",
      "\n",
      "누나랑 연락하는 거 좋아요! @@\n"
     ]
    }
   ],
   "source": [
    "validset = tokenized_dataset['valid'].with_format(\"torch\", device=device)\n",
    "test_sample = validset.shuffle().select(range(1))\n",
    "print(test_sample['input_ids'].shape)\n",
    "output = generate_from_data(model, tokenizer, test_sample)\n",
    "\n",
    "print(output['input_text'], output['generated_text'], output['target_text'], sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(validset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:48<00:00,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7985125476607406\n",
      "0.5424880093483264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'numpy.float64'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m average_rouge_score \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(rouge_score_lt)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(average_bleu_score, average_rouge_score, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m pd\u001b[39m.\u001b[39;49mconcat([average_bleu_score, average_rouge_score], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:294\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m     92\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    102\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FrameOrSeriesUnion:\n\u001b[1;32m    103\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    295\u001b[0m         objs,\n\u001b[1;32m    296\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    297\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    298\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    299\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    300\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    301\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    302\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    303\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    304\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:384\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[1;32m    380\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    381\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcannot concatenate object of type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obj)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39monly Series and DataFrame objs are valid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m         )\n\u001b[0;32m--> 384\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    386\u001b[0m     ndims\u001b[39m.\u001b[39madd(obj\u001b[39m.\u001b[39mndim)\n\u001b[1;32m    388\u001b[0m \u001b[39m# get the sample\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39m# want the highest ndim that we have, and must be non-empty\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39m# unless all objs are empty\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'numpy.float64'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "accuracy_lt = []\n",
    "bleu_score_lt = []\n",
    "rouge_score_lt = []\n",
    "accuracy = evaluate.load('accuracy')\n",
    "bleu = evaluate.load('google_bleu')\n",
    "#rouge = evaluate.load('rouge')\n",
    "rouge = Rouge()\n",
    "\n",
    "for example in tqdm(validset.shuffle().select(range(2000))):\n",
    "    data = dict()\n",
    "    for key in example:\n",
    "        data[key] = example[key].unsqueeze(0)\n",
    "    output = generate_from_data(model, tokenizer, data)\n",
    "    generated_text = output['generated_text']\n",
    "    target_text = output['target_text']\n",
    "\n",
    "    try:\n",
    "        #accuracy_score = accuracy.compute(predictions=generated_text, references=target_text, tokenizer=tokenizer)\n",
    "        #bleu_score = bleu.compute(predictions=generated_text, references=target_text, tokenizer=tokenizer)\n",
    "        bleu_score = sentence_bleu([target_text], generated_text, smoothing_function=SmoothingFunction().method1)\n",
    "        rouge_score = rouge.get_scores(generated_text, target_text)[0]['rouge-2']['f']\n",
    "        #rouge_score = rouge.compute(predictions=generated_text, references=target_text)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    #accuracy_lt.append(accuracy_score)\n",
    "    bleu_score_lt.append(bleu_score)\n",
    "    rouge_score_lt.append(rouge_score)\n",
    "\n",
    "bleu_score_series = pd.Series(bleu_score_lt)\n",
    "rouge_score_series = pd.Series(rouge_score_lt)\n",
    "print(bleu_score_series.mean(), rouge_score_series.mean(), sep='\\t')\n",
    "result_df = pd.concat([bleu_score_series, rouge_score_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
