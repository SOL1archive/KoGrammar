{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp31SzoGfn6c",
        "outputId": "95dc27c0-19f4-4561-d196-27a5408ad962"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "IN_COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsKGqu5qyBd9",
        "outputId": "16da264d-8d64-46f6-8234-a3b8ff6e6da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    !pip install transformers\n",
        "    !pip install datasets\n",
        "    !pip install evaluate\n",
        "    !pip install rouge_score\n",
        "    !pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBDkVI4byO_Z",
        "outputId": "8e6dc17a-37e7-4f4b-e97a-59c20d27f580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "wS60niTbxf14"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import datetime\n",
        "import os\n",
        "import gc\n",
        "from collections import namedtuple\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorboard\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "import torchmetrics\n",
        "\n",
        "from datasets import load_dataset, load_from_disk, DatasetDict, Dataset\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import BartConfig, T5Config\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import evaluate\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTxXx3eqxf17"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bzZ92td4xf18"
      },
      "outputs": [],
      "source": [
        "MANUAL_TRAINING = True\n",
        "MANUAL_VALIDATION = True\n",
        "NUM_EPOCHS = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQJN1Bg2xf19"
      },
      "source": [
        "## Loading Tokenizer & Model Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olYwPxAVxf19",
        "outputId": "5b07b074-0da5-4154-c28d-00a8b9cad7ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Checkpoint: gogamza/kobart-base-v2\n"
          ]
        }
      ],
      "source": [
        "kobart_checkpoint = 'gogamza/kobart-base-v2'\n",
        "kot5_checkpoint = 'psyche/KoT5'\n",
        "kobart_baseline_checkpoint = ''\n",
        "checkpoint = kobart_checkpoint\n",
        "print(f'Using Checkpoint: {checkpoint}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wSJNirpxf19",
        "outputId": "f3f26e2c-34be-470d-c9a1-aacd66f3e2e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ],
      "source": [
        "if checkpoint == kobart_checkpoint:\n",
        "    config = BartConfig.from_pretrained(kobart_checkpoint)\n",
        "    #config['vocab'] = 30000\n",
        "else:\n",
        "    config = T5Config.from_pretrained(kot5_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbJ90HZ9xf1-",
        "outputId": "e0e6a70d-a43d-4d33-cc68-ccb4c743e2f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, \n",
        "                                          max_length=512, \n",
        "                                          truncation=False, \n",
        "                                          padding='max_length',\n",
        "                                          #vocab=config.vocab_size\n",
        "                                          )\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gZVm_OoUxf1-"
      },
      "outputs": [],
      "source": [
        "if len(tokenizer) != model.config.vocab_size:\n",
        "    raise RuntimeError(f'Tokenizer vocab size and model vocab size do not match(Tokenizer:{len(tokenizer)} Model: {model.config.vocab_size}). Which would lead to further error in training.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPEV9Errxf1_"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWfNQdD-xf1_",
        "outputId": "d4f57b2e-bdca-4a61-a657-bd226f7255e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1129363"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = Dataset.from_pandas(pd.read_json('drive/MyDrive/projects/KoGrammar/data/simplified_data.json'))\n",
        "\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-FoXyrxcxf1_"
      },
      "outputs": [],
      "source": [
        "train_testvalid = dataset.train_test_split(test_size=0.1)\n",
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'valid': test_valid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4LWQzF0nxf1_"
      },
      "outputs": [],
      "source": [
        "def tokenize(row):\n",
        "    form_embeddings = tokenizer(row['form'], max_length=512, truncation=True, padding='max_length')\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        correct_form_embeddings = tokenizer(row['corrected_form'], max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    return {\n",
        "        'input_ids': form_embeddings['input_ids'],\n",
        "        'attention_mask': form_embeddings['attention_mask'],\n",
        "        'labels': correct_form_embeddings['input_ids'],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWa-jbFNxf2A",
        "outputId": "03f3ce99-5cf8-45d1-b534-93ee47a1e30b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['train', 'valid', 'test'])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FivIB61k-mYK",
        "outputId": "967248ff-0804-4551-fe21-bb2fd980b64b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PMoHdJyU-hv6"
      },
      "outputs": [],
      "source": [
        "replaced_checkpoint = checkpoint.replace('/', '-')\n",
        "tokenized_dataset_path = f'drive/MyDrive/projects/KoGrammar/data/{replaced_checkpoint}_tokenized_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_C5WwBW9L11",
        "outputId": "f8acf371-ac01-4dec-81f8-0b0ce39fc56c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.exists(tokenized_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7U80NEZRxf2A"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(tokenized_dataset_path):\n",
        "    tokenized_dataset = (dataset_dict\n",
        "                         .map(tokenize, \n",
        "                              #batched=True, \n",
        "                              #batch_size=512, \n",
        "                              num_proc=10\n",
        "                              )\n",
        "                         .remove_columns(['form', 'corrected_form'])\n",
        "                         )\n",
        "    \n",
        "    tokenized_dataset.save_to_disk(tokenized_dataset_path)\n",
        "else:\n",
        "    tokenized_dataset = load_from_disk(tokenized_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCqgpwAPxf2A",
        "outputId": "c1eb2dbd-ab9e-4e8d-8c3b-847e6e515ef6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'__index_level_0__': 550838,\n",
              " 'input_ids': [14250,\n",
              "  12332,\n",
              "  12926,\n",
              "  12034,\n",
              "  10746,\n",
              "  12709,\n",
              "  22251,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'labels': [14250,\n",
              "  12332,\n",
              "  12926,\n",
              "  12034,\n",
              "  14097,\n",
              "  12710,\n",
              "  17687,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset['train'][10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFE38d66xf2B"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_OiPtuWLxf2B"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t9sDCjflxf2B"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"tensorboard\",\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6iZRgNv1xf2B"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_dict['train'],\n",
        "    eval_dataset=dataset_dict['valid'],\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "xXEsxMJgxf2B",
        "outputId": "b520e0bb-fed2-4dd2-bb05-389c9a181799"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|         | 3386/31764 [1:32:51<12:58:14,  1.65s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2e809607f30d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if not MANUAL_TRAINING:\n",
        "    trainer.train()\n",
        "else:\n",
        "    total_loss_lt = []\n",
        "    batch_loss_lt = []\n",
        "\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    trainset = tokenized_dataset['train'].with_format(\"torch\", device=device)\n",
        "    dataloader = DataLoader(trainset, \n",
        "                            batch_size=32, \n",
        "                            shuffle=True, \n",
        "                            #collate_fn=lambda lt: pad_sequence(lt, \n",
        "                            #                                   batch_first=True, \n",
        "                            #                                   padding_value=tokenizer.pad_token_id\n",
        "                            #                                   )\n",
        "                            )\n",
        "    if not next(model.parameters()).is_cuda and device == torch.device('cuda'):\n",
        "        model.to(device)\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        for batch in tqdm(dataloader):\n",
        "            X = {\n",
        "                    'input_ids': batch['input_ids'],\n",
        "                    'attention_mask': batch['attention_mask'],\n",
        "                }\n",
        "            y = batch['labels']\n",
        "            outputs = model(**X, labels=y)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            batch_loss_lt.append(loss.item())\n",
        "\n",
        "        total_loss_lt += batch_loss_lt\n",
        "        batch_loss_series = pd.Series(batch_loss_lt)\n",
        "        print(f'epoch {epoch + 1} loss: {loss.item()} mean: {batch_loss_series.mean()}')\n",
        "    '''\n",
        "    except:\n",
        "        print(\n",
        "            'input_ids: ' + str(X['input_ids'].shape), \n",
        "            'attention_mask: ' + str(X['attention_mask'].shape), \n",
        "            'labels: ' + str(y.shape), \n",
        "            sep='\\t'\n",
        "        )\n",
        "        '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhdYVByJzLrv"
      },
      "outputs": [],
      "source": [
        "total_loss_series = pd.Series(total_loss_lt)\n",
        "total_loss_series.plot.line()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_GjLcX4xf2C"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyKkshNNxf2C",
        "outputId": "45175739-1045-4538-9d1e-fd9dfa0a5471"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /content/drive/MyDrive/projects/KoGrammar/data/gogamza-kobart-base-v2_tokenized_dataset/valid/cache-017865f025dd26d3.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "\n",
            ",   .\n",
            "\n",
            ",   .\n"
          ]
        }
      ],
      "source": [
        "validset = tokenized_dataset['valid'].with_format(\"torch\", device=device)\n",
        "test_sample = validset.shuffle().select(range(1))\n",
        "test_sample_gt = test_sample['labels']\n",
        "test_sample = test_sample.remove_columns('labels')[0]\n",
        "test_sample_input = dict()\n",
        "test_sample_input['input_ids'] = test_sample['input_ids'].unsqueeze(0)\n",
        "test_sample_input['attention_mask'] = test_sample['attention_mask'].unsqueeze(0)\n",
        "output = model.generate(**test_sample_input)\n",
        "input_text = tokenizer.decode(test_sample_input['input_ids'].squeeze(0), skip_special_tokens=True)\n",
        "output_text = tokenizer.decode(output.squeeze(0), skip_special_tokens=True)\n",
        "gt_text = tokenizer.decode(test_sample_gt.squeeze(0), skip_special_tokens=True)\n",
        "\n",
        "print(input_text, output_text, gt_text, sep='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMKMmVcgxf2C"
      },
      "outputs": [],
      "source": [
        "if not MANUAL_VALIDATION:\n",
        "    trainer.evaluate(dataset_dict['valid'])\n",
        "else:\n",
        "    loss_lt = []\n",
        "\n",
        "    model.eval()\n",
        "    validset = tokenized_dataset['valid'].with_format(\"torch\", device=device)\n",
        "    dataloader = DataLoader(validset, batch_size=1, shuffle=True)\n",
        "    if not next(model.parameters()).is_cuda and device == torch.device('cuda'):\n",
        "        model.to(device)\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                X = {\n",
        "                        'input_ids': batch['input_ids'],\n",
        "                        'attention_mask': batch['attention_mask'],\n",
        "                    }\n",
        "                y = batch['labels']\n",
        "                outputs = model(**X, labels=y)\n",
        "                loss = outputs.loss\n",
        "                loss_lt.append(loss.item())\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    loss_series = pd.Series(loss_lt)\n",
        "    print(f'loss: {loss_series.mean()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gyvaAcx7a7b7"
      },
      "outputs": [],
      "source": [
        "def eval(model, tokenizer, input_seq, target_seq, metric):\n",
        "    generated_ids = model.generate(**input_seq)\n",
        "    generated_sentence = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    target_sentence = tokenizer.decode(target_seq, skip_special_tokens=True)\n",
        "    score = metric.compute(generated_sentence, target_sentence)\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "Rge4CI8PZUZf",
        "outputId": "9e30875a-4209-49f8-f279-bc73bfd0ee3e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /content/drive/MyDrive/projects/KoGrammar/data/gogamza-kobart-base-v2_tokenized_dataset/valid/cache-a55939bfbb504316.arrow\n",
            "  0%|          | 1/56468 [00:00<8:18:17,  1.89it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-51d6373bd397>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#bleu_score = bleu.compute(predictions=generated_sentence, references=target_sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mrouge_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerated_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mbleu_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m                     \u001b[0;34mf\"Input references: {summarize_if_long_list(references)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m                 )\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of predictions (21) and references (23)"
          ]
        }
      ],
      "source": [
        "bleu_scores = []\n",
        "rouge_scores = []\n",
        "bleu = evaluate.load('bleu')\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "for example in tqdm(validset.shuffle()):\n",
        "    input_sentence = {\n",
        "                        'input_ids': example['input_ids'].unsqueeze(0),\n",
        "                        'attention_mask': example['attention_mask'].unsqueeze(0),\n",
        "                     }\n",
        "    \n",
        "    target_seq = example['labels']\n",
        "    generated_ids = model.generate(**input_sentence)\n",
        "    generated_sentence = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    target_sentence = tokenizer.decode(target_seq, skip_special_tokens=True)\n",
        "    \n",
        "    #bleu_score = bleu.compute(predictions=generated_sentence, references=target_sentence)\n",
        "    rouge_score = rouge.compute(predictions=generated_sentence, references=target_sentence)\n",
        "\n",
        "    #bleu_scores.append(bleu_score)\n",
        "    rouge_scores.append(rouge_score)\n",
        "\n",
        "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "print(f\"Average BLEU score: {average_bleu_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cX6bIDNxf2C"
      },
      "source": [
        "## Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmoYzjEbxf2C"
      },
      "outputs": [],
      "source": [
        "# To prevent unwanted saves\n",
        "raise RuntimeError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ2uGG5Jxf2C"
      },
      "outputs": [],
      "source": [
        "NOW_STR = datetime.datetime.now().strftime('%y%m%d-%H:%M')\n",
        "trainer.create_model_card(\n",
        "    language='Korean',\n",
        "    tags='Grammar',\n",
        "    #model='KoGrammar',\n",
        "    finetuned_from=checkpoint\n",
        ")\n",
        "trainer.save_model(f\"drive/MyDrive/projects/KoGrammar/models/{NOW_STR}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
